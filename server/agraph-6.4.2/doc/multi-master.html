<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="author" content="Franz Incorporated"/>
<title>Multi-master Replication in AllegroGraph</title>
<link rel='stylesheet' href='jquery-ui.custom.min.css' />
<link rel='stylesheet' href='stylesheet.css' />
<link rel='stylesheet' href='print.css' media='print' />
<body id="multi-master"> <div id="header"> <p>                 </p>                <p><script src="jquery.min.js" type="text/javascript" charset="utf-8"></script> <script src="jquery-ui.custom.min.js" type="text/javascript" charset="utf-8"></script> <script src="activebookmark.js" type="text/javascript" charset="utf-8"></script> </p> <div id='search-form'>   <form method="GET" action="https://www.google.com/search">     <input type="hidden" name="as_sitesearch" value=""> </input>     <input type="text" size="40" name="as_q" value="" placeholder="Please enable JavaScript to use search!"> </input>&nbsp;     <input type="submit" value="Search" disabled> </input>   </form> </div> <script>    /* Docudown has no support for function calls in HTML attributes. 
<ul>
<li>Thus, we set the as_sitesearch attribute using JavaScript below.    */   (function () { 'use strict'; var properValue = "franz.com/agraph/support/documentation/6.4.2/"; var input = document.querySelector(   '#search-form input[name="as_sitesearch"]'); input.setAttribute('value', properValue); /* The submit button was disabled so that search would be 
<ul>
<li>impossible for users without JavaScript. Enabling now: */ document.querySelector('#search-form input[type="submit"]').   removeAttribute('disabled'); document.querySelector('#search-form input[name="as_q"]').   removeAttribute('placeholder');   })(); </script> <div id="copyright">  Copyright (c) 2005 - 2018 Franz, Incorporated</div> <div id="timestamp">  Last updated 8 June 2018 at 06:55</div> <a href="https://franz.com" alt="Franz Inc Home Page"><div id="hdLogo"> </div></a> </li></ul></li></ul>       <h1>Multi-master Replication in AllegroGraph 6.4.2</h1></div> <div id="contents">  <div id="docIndex"> <ul id="agmenu"> <li><a href="index.html">Documentation Index</a> 
<ul>
<li><a href="release-notes.html">Release Notes</a></li>
<li><a href="agraph-quick-start.html">Quick Start</a></li>
<li><a href="agraph-introduction.html" id="agraph-introduction-tab">Introduction</a></li>
<li>Setup and Configuration
<ul>
<li><a href="http://franz.com/agraph/downloads/" target="_blank">Download</a></li>
<li><a href="server-installation.html">Server Installation</a></li>
<li><a href="daemon-config.html">Server Configuration and Control</a></li>
<li><a href="agwebview.html">WebView</a></li>
<li><a href="upgrade-guide.html">Database Upgrading</a></li></ul></li>
<li>Server Management
<ul>
<li><a href="performance-tuning.html">Performance Tuning</a></li>
<li><a href="deleting-duplicate-triples.html">Deleting Duplicate Triples</a></li>
<li><a href="purging-deleted-triples.html">Purging Deleted Triples</a></li>
<li><a href="audit.html">Auditing</a></li>
<li><a href="managing-users.html">Managing Users</a></li>
<li><a href="replication.html">Replication</a></li>
<li><a href="point-in-time-recovery.html">Point-in-Time Recovery</a></li>
<li><a href="transaction-log-archiving.html">Transaction Log Archiving</a></li>
<li><a href="security-overview.html">Security Overview</a></li>
<li><a href="security.html">Security Implementation</a></li></ul></li>
<li>Tools
<ul>
<li><a href="agtool.html">agtool</a></li>
<li><a href="agload.html">Data Loading</a></li>
<li><a href="agexport.html">Data Export</a></li>
<li><a href="backup-and-restore.html">Backup and Restore</a></li>
<li><a href="agquery.html">Querying</a></li>
<li><a href="reasoner-tutorial.html" id="reasoner-tutorial-tab">RDFS++</a></li>
<li><a href="materializer.html">Materializer</a></li></ul></li>
<li>Details
<ul>
<li><a href="triple-index.html">AllegroGraph Indices</a></li>
<li><a href="text-index.html">Full-text Indices</a></li>
<li><a href="encoded-ids.html">Encoded IDs</a></li>
<li><a href="connection-pooling.html">Connection Pooling</a></li>
<li><a href="geospatial-nd.html">N-dimensional Geospatial Overview</a></li>
<li><a href="magic-properties.html#sparql-magic-geo-2d">2-D Geospatial</a></li>
<li><a href="magic-properties.html#sparql-magic-temporal">Temporal</a></li>
<li><a href="magic-properties.html#sparql-magic-sna">Social Network</a></li>
<li><a href="javascript.html">JavaScript</a></li>
<li><a href="datatypes.html">Datatypes</a></li>
<li><a href="rapper.html">Data Conversion (Rapper)</a></li>
<li><a href="stored-procedures.html">Stored Procedures</a></li>
<li><a href="triple-attributes.html">Triple Attributes</a></li></ul></li>
<li>Querying
<ul>
<li><a href="sparql-overview.html">SPARQL documentation summary</a></li>
<li><a href="sparql-reference.html">SPARQL Reference</a></li>
<li><a href="spin.html">SPIN</a></li>
<li><a href="magic-properties.html">SPARQL Magic Properties</a></li>
<li><a href="prolog-tutorial.html">Prolog</a></li></ul></li>
<li>3rd-party tools
<ul>
<li><a href="solr-index.html">Solr text Indices</a></li>
<li><a href="mongo-interface.html">MongoDB integration</a></li>
<li><a href="TBCplugin.html">TopBraid Composer Plugin</a></li>
<li><a href="agraph-introduction.html#othertools">Cloudera</a></li></ul></li>
<li>Client APIs
<ul>
<li><a href="http://franz.com/agraph/gruff/" title="Gruff" target="_blank">Gruff</a></li>
<li><a href="http-protocol.html">REST/HTTP interface</a></li>
<li><a href="http-reference.html">HTTP reference</a></li>
<li><a href="javadoc/index.html">Javadocs (Sesame and Jena)</a></li>
<li><a href="python/index.html">Python API</a></li>
<li><a href="lisp-reference.html">Lisp Reference</a></li>
<li><a href="javascript.html">JavaScript</a></li></ul></li>
<li>Tutorials
<ul>
<li><a href="geospatial-nd-tutorial.html">N-dimensional Geospatial</a></li>
<li><a href="sparql-tutorial.html">SPARQL Tutorial</a></li>
<li><a href="java-tutorial/java-tutorial.html">Java Sesame</a></li>
<li><a href="java-tutorial/jena-tutorial.html">Java Jena</a></li>
<li><a href="python/tutorial.html">Python Sesame</a></li>
<li><a href="lisp-quickstart.html">Lisp</a></li>
<li><a href="prolog-tutorial.html">Prolog</a></li>
<li><a href="tutorial-index.html">More...</a></li></ul></li>
<li>Other
<ul>
<li><a href="suggested-reading.html">Suggested Reading</a></li>
<li><a href="http://franz.com/ps/services/conferences_seminars/" title="Conferences and Seminars" target="_blank">Conferences and Seminars</a></li>
<li><a href="http://github.com/franzinc">Source Code on Github</a></li>
<li><a href="http://agraph.franz.com/cresources/index.lhtml" target="_blank">Community Resources</a></li>
<li><a href="http://franz.com/agraph/ec2/">Amazon EC2</a></li>
<li><a href="copyrights.html">Copyrights</a></li>
<li><a href="change-history.html" id="change-history-tab">Change History</a></li></ul></li>
<li><a href="http://franz.com/" target="_blank">Franz, Inc.</a> </li> </ul> </li></ul><p><script> $(function() { $( "#agmenu" ).menu(); }); </script> </p></div>   
<div class='table-of-contents' id='table-of-contents'>

<ul>
<li><a href='#header2-8' title='Introduction'>Introduction</a></li>
<li><a href='#header2-13' title='Setting up a multi-master cluster'>Setting up a multi-master cluster</a>
<ul>
<li><a href='#header3-21' title='Setting up a multi-master cluster using AGWebView'>Setting up a multi-master cluster using AGWebView</a></li>
<li><a href='#header3-26' title='Using AGWebView to convert a repository to a multi-master instance'>Using AGWebView to convert a repository to a multi-master instance</a></li>
<li><a href='#header3-52' title='The controlling instance'>The controlling instance</a></li>
<li><a href='#header3-61' title='Managing clusters in AGWebView once created'>Managing clusters in AGWebView once created</a></li>
<li><a href='#header3-132' title='Instance Settings'>Instance Settings</a></li>
<li><a href='#header3-164' title='Cluster operations you cannot do with AGWebView'>Cluster operations you cannot do with AGWebView</a></li></ul></li>
<li><a href='#header2-170' title='Using the agtool repl command'>Using the agtool repl command</a>
<ul>
<li><a href='#header3-188' title='The SPEC argument to agtool repl'>The SPEC argument to agtool repl</a></li>
<li><a href='#header3-195' title='Using agtool to set the controlling instance'>Using agtool to set the controlling instance</a></li>
<li><a href='#header3-200' title='Using agtool to create a cluster'>Using agtool to create a cluster</a></li>
<li><a href='#header3-238' title='Using agtool to add an instance to a cluster'>Using agtool to add an instance to a cluster</a></li>
<li><a href='#header3-244' title='Using agtool to specify cluster instance priorities'>Using agtool to specify cluster instance priorities</a></li>
<li><a href='#header3-252' title='Using agtool to quiesce a cluster so all instances sync up'>Using agtool to quiesce a cluster so all instances sync up</a></li>
<li><a href='#header3-268' title='Using agtool to remove a cluster instance'>Using agtool to remove a cluster instance</a></li>
<li><a href='#header3-273' title='Using agtool to specify the transaction log retention period'>Using agtool to specify the transaction log retention period</a></li>
<li><a href='#header3-286' title='Using agtool to set cluster parameters'>Using agtool to set cluster parameters</a></li>
<li><a href='#header3-300' title='Using agtool to start an instance'>Using agtool to start an instance</a></li>
<li><a href='#header3-305' title='Using agtool to get the status of an instance'>Using agtool to get the status of an instance</a></li>
<li><a href='#header3-310' title='Using agtool to stop a cluster instance'>Using agtool to stop a cluster instance</a></li></ul></li>
<li><a href='#header2-315' title='Details on Certain Multi-master Replication Features'>Details on Certain Multi-master Replication Features</a>
<ul>
<li><a href='#header3-318' title='Repository priorities'>Repository priorities</a></li></ul></li>
<li><a href='#header2-347' title='The REST interface for multi-master replication'>The REST interface for multi-master replication</a></li></ul>
<div id='tocLink'><a href='index.html'>Documentation Index</a></div>
</div>
  <div id="main-content"> <a name='header2-8' id='header2-8'></a><h2>Introduction</h2><p>AllegroGraph Multi-master Replication is a real-time transactionally consistent data replication solution. It allows businesses to move and synchronize their semantic data across the enterprise. This facilitates real-time reporting, load balancing, and disaster recovery. </p><p>Single repositories can be replicated as desired. The replicas each run in an AllegroGraph server. A single server can serve multiple replicas of the same repository (this is not typical for production work but might be common in testing). Note if there are multiple replicas in a single server, each replica must either be in a different catalog or must have a different name. </p><p>The collection of servers with replicas of a particular repository is called a <em>replication cluster</em> (or just <em>cluster</em> below in this document). Each repository in the cluster is called an <em>instance</em>. One instance is designated as the <em>controlling instance</em>, which will be described in more details below. </p><p>Each instance in the cluster can add or delete triples and these additions and deletions are passed to all other instances in the cluster. How long it takes for instances to synchronize depends on factors external to AllegroGraph (such as network availability and speed and whether the other servers are even available) but given time and assuming all instances are accessible, after a period of no activity (no additions or deletions) all instances will become synchronized. </p><a name='header2-13' id='header2-13'></a><h2>Setting up a multi-master cluster</h2><p>You can start multi-master replication cluster on a repository using AGWebView, using the <a href="#agtool-repl">agtool repl</a> command, or using the <a href="#rest-interface">REST interface</a>. We will discuss using AGWebView and <strong>agtool repl</strong> in greatest detail. </p><p>A cluster is created either by using <strong>agtool repl create-cluster</strong> to create a new repository as a cluster instance (that operation cannot be done in AGWebView) or by starting with an initial instance of a repository (which may be an empty repository or may already contain triples) and then cloning it. The clones can be on the same server or on different servers. </p><p>When you convert an existing repository to a cluster instance, the repository must not be open. Both AGWebView and <strong>agtool repl create-cluster</strong> close the repository before conversion. If it is not possible for any reason to close the repository, the conversion will fail. </p><p>The cluster can be grown at any time by adding additional instances and it can be shrunk by removing instances. </p><p>The first step to setting up a multi-master cluster is to specify a repository in some catalog on some server as the first instance of a cluster. The repository must be an existing repository (which may have just been created for this purpose). Once it has been converted to be the first instance of a replication cluster, other instances can created and added to the cluster. (Existing repositories cannot be added to an existing cluster. Each instance added after the initial creation of a cluster will be newly created when added.) </p><p>Newly created instances can have their own names and can be in their own catalogs.  So the initial instance can be named <em>kennedy</em> and be in the root catalog of <em>server1</em>, a second instance can be named <em>kennedy-dup</em> and also be in the root catalog of <em>server1</em>, a third named <em>kennedy</em> in the catalog <em>mycatalog</em> of <em>server1</em>, a fourth named <em>kennedy</em> in the root catalog of <em>server2</em>, and so on. </p><p><a name="using-webview"></a> </p><a name='header3-21' id='header3-21'></a><h3>Setting up a multi-master cluster using AGWebView</h3><p>To create a multi-master replication cluster in AGWebView, you must start with an existing repository in the server connected to by the AGWebView screen. You can use an existing repository (that is not already part of any cluster) or you can create a new repository using the tool on the <a href="agwebview.html#RootCatalogPage">catalog page in AGWebView</a> (the link is to the description of the Root catalog page). </p><p><img src="agwebview_catalogpage.jpg" alt="Root Catalog Page"></img> </p><p>There you will see repositories already existing in the catalog, along with tools to create a new ordinary repository and (unrelated to the discussion here) to start a session. Choose an existing repository to make the first instance of a cluster or create a new one for that purpose. We have a <em>kennedy</em> repository and we will use that one. Clicking on the repository name will take you to the repository overview page, described next. </p><p><a name="convert-using-webview"></a> </p><a name='header3-26' id='header3-26'></a><h3>Using AGWebView to convert a repository to a multi-master instance</h3><p>Here is a portion of the <a href="agwebview.html#RepositoryOverviewPage">Repository Overview</a>: </p><p><img src="repo-page-convert-store.jpg" alt="Repo page showing Convert store to replication instance"></img> </p><p>The red arrow points to the link <strong>Convert store to a replication instance</strong>. Clicking on this link displays the following dialog on the AGWebView page: </p><p><img src="create-cluster-wv-dialog.jpg" alt="Dialog to convert an ordinary repository to a cluster instance"></img> </p><p>The information you will supply in the various fields identifies the instance being created and specifies how other instances will contact the new instance. You have to decide whether they will use HTTP or HTTPS, and what the host and port are. Just the <strong>Scheme</strong>, <strong>Host</strong>, and the <strong>Port</strong> fields will be filled in initially (we have filled in the remainder as an illustration). The fields are: </p>
<dl><dt><strong>Scheme</strong> </dt>
<dd><em>http</em> or <em>https</em>.</dd><dt><strong>Host</strong> </dt>
<dd><p>The server containing the repository which is to be made into the first instance of the new cluster runs on a specific host. That host is specified in this field and a suggested value is provided. The value must be specified in a way that allows access from any server that will contain a repository in the cluster. The value can be a hostname (such as the suggested <code>crow.franz.com</code>) or an IP address. You should avoid the hostname <code>localhost</code> as that will not work properly for access from other machines. (<code>localhost</code> can be used when all cluster repositories will be in servers running on a single machine, which can be useful for testing but is unusual in production environments.) </p></dd>
<dd><p>Because you are creating the first instance of a new cluster, the host must be the machine running the server in which the existing repository being converted resides, and the port must be a port on that machine which communicates with that server. </p></dd><dt><strong>Port</strong> </dt>
<dd>The current port is filled in but you may wish to use another port (such as a port for HTTPS). </dd><dt><strong>Catalog</strong> </dt>
<dd>This field as shown is not settable since the repository already is in some catalog (such as, in this case, the root catalog) and that cannot be changed by the conversion operation. (The dialog for creating a new instance is very similar but the catalog field is active in that dialog since the new repository instance can have a different name and be in a different catalog.) </dd><dt><strong>Repository Name</strong> </dt>
<dd>This is filled in with the name of the repository being converted (<code>kennedy</code> in this example). </dd><dt><strong>Group</strong> </dt>
<dd>If supplied, the new instance will be in the named group. Groups and the associated notion of <em>priority</em> are described in the section <a href="#repo-priority">Repository priorities</a> below. It is not necessary to include a repository instance in a group and this field can be left blank. Group names must be distinct from instance names. </dd><dt><strong>Username</strong> and <strong>Password</strong> </dt>
<dd>This is the username and password that will be used by other replication instances to contact the replication instance being created. It must be a username with superuser privileges. </dd><dt><strong>Instance Name</strong> </dt>
<dd>Each instance of a cluster must have a unique (to the cluster) name. (It will also have an instance ID number assigned by the system.) This allows easy reference to the instance. You can leave this field blank and the system will select a name. </dd></dl><p>When the form is filled in, click on the <strong>Convert Store to Instance</strong> button and the conversion will take place. </p><p><strong>Problem with encoded-ids</strong>. If the repository being converted has encoded id templates defined, the conversion can be done even if those templates do not allow for 2^60 unique identifiers but trying to get the next encoded upi for that prefix will fail. See <a href="#encoded-ids">Issues with encoded-ids</a> below for more information. </p><p><a name="controlling-instance"></a> </p><a name='header3-52' id='header3-52'></a><h3>The controlling instance</h3><p>In a replication cluster, one instance is distinguished as the <strong>controlling instance</strong>. When a cluster is created, the first instance becomes the controlling instance. Later, control can be shifted to other instances. </p><p>Certain operations can only be done by the controlling instance. These special operations are those that if done simultaneously on multiple instances would cause conflicts to exist. These operations are: </p>
<ul>
<li><p>Adding another instance to the cluster.</p></li>
<li><p>Removing an existing instance from the cluster.</p></li>
<li><p>Defining or deleting a triple attribute (see <a href="triple-attributes.html">Triple   Attributes</a>) </p></li>
<li><p>Defining an encoded id.</p></li></ul><p>You can change the controlling instance. This is typically done (using <strong>agtool repl</strong> or the AGWebView <strong>Replication Cluster Management</strong> page) by sending a command to the current controlling instance. Making the change this way ensures that only one instance believes it is the controlling instance. The exception is when the controlling instance is unavailable (because, for example, its server is down). Then another instance can be made the controlling instance but then, for a while at least, when the former controlling instance becomes available, there will be two instances thinking they are controlling. Care should be used when that situation occurs. </p><p><a name="managing-clusters-webview"></a> </p><a name='header3-61' id='header3-61'></a><h3>Managing clusters in AGWebView once created</h3><p><img src="replication-control-wv-link.jpg" alt="Repository page with Replication Control link"></img> </p><p>When a repository is an instance of a cluster, its AGWebView repository page contains a <strong>Replication Control</strong> section with a <strong>Manage Replication Instances [as controller]</strong> link (<em>as controller</em> only appears if the instance is the controlling instance, but the section appears, with at least a link to <strong>Manage Replication Instances</strong> on the repository pages for all instances in the cluster). </p><p>Clicking on the link takes you to the <strong>Replication Cluster Management</strong> page, shown (for a cluster with only one instance which is thus the controlling instance) below. </p><p><img src="cluster-manage-one-inst.jpg" alt="Replication Cluster Management page"></img> </p><p>We will describe the various columns, buttons and graphs shortly, but first we add an instance by clicking on the <strong>Add Instance</strong> button. This dialog appears: </p><p><img src="new-cluster-instance-dialog-wv.jpg" alt="Add Cluster Instance Dialog"></img> </p><p>You fill in the various fields. Here are some notes: </p>
<dl><dt><strong>Host</strong> </dt>
<dd>No suggested value is provided because the system has no way of knowing what host you want. The value should be a hostname or a IP address. Avoid <code>localhost</code> unless all instances will be in servers running on the same host (not uncommon when testing but rare in production). </dd><dt><strong>Port</strong> </dt>
<dd>This field is filled in with the port used by the current server. Be sure to change this to be the port used by the target server. (The current server can host both instances if either their names or catalogs are different. More typically, instances are in different servers.) </dd><dt><strong>Catalog</strong> and <strong>Repository Name</strong> </dt>
<dd>These can be different on the target server than on the current server. The catalog specified must exist on the target server (the root catalog, the default if no catalog is specified, always exists). The repository name can be the name of an existing repository, but (as the warning at the top says) that repository will in that case be destructively replaced with all its data being lost. </dd><dt><strong>Group</strong> </dt>
<dd>If supplied, the new instance will be in the named group. Groups and the associated notion of <em>priority</em> are described in the section <a href="#repo-priority">Repository priorities</a> below. Some groups, created when other instances were created, may already exist. You can include this instance in an existing group, or in a new group, or in no group (leave this field blank if you do not want the instance to be in any group). Group names must be distinct from instance names. </dd></dl><p>The <strong>Instance Name</strong> must be a unique (in this cluster) name (the system will choose a suitable name if none is provided). </p><p><strong>Username</strong> and <strong>Password</strong> must be for a user on the server identified by <strong>Host</strong> and <strong>Port</strong> who has superuser privileges. </p><p>We now have two cluster instances in two servers. The Replication Cluster Management table for the controlling instance now looks like this: </p><p><img src="cluster-manager-pict.jpg" alt="Controlling Cluster Manager"></img> </p><p>It shows the two instances. Below the instance table is a log describing recent actions, adding an instance in this case. The various steps are shown when completed. Problems, if any (none in this case) are also noted. The graphs are still below but have been left out of this picture. </p><p>The page for non-controlling instances looks like this: </p><p><img src="non-control-cluster-manager.jpg" alt="Non-controlling Cluster Manager"></img> </p><p>Note the <strong>Transaction Rate</strong> graph had a burst of activity when the instance was created. </p><p>The information table is similar in both cases but the controlling instance page has many control buttons while the non-controlling page has just two. </p><p>The columns in both are: </p>
<dl><dt><strong>Selection Tool</strong> (a check mark) </dt>
<dd>If the checkbox in this column is checked, various button options become live and clicking the button affects the selected instances. </dd><dt><strong>Instance ID</strong> </dt>
<dd>The value is the instance ID number assigned by the system to each instance.</dd><dt><strong>Instance Name</strong> </dt>
<dd>The given name for each instance (chosen when the instance was created).</dd><dt><strong>Repository</strong> </dt>
<dd>The repository name for the particular instance. (These are both <code>kennedy</code> in the example but they can be different.) </dd><dt><strong>Group</strong> </dt>
<dd>The group of the instance, if any. Groups and the associated notion of <em>priority</em> are described in the section <a href="#repo-priority">Repository priorities</a> below. </dd><dt><strong>State</strong> </dt>
<dd>The current instance state (running, unreachable, etc.) An instance is unreachable if it cannot be contacted. That can happen when the instance is stopped or when the machine or the AllegroGraph server on which the instance would run is itself not running. </dd><dt><strong>Controlling</strong> </dt>
<dd>The controlling instance has 'yes' in this column. There is only one controlling instance. See <a href="#controlling-instance">below</a> for information on the controlling instance. </dd><dt><strong>This Instance</strong> </dt>
<dd>The AGWebView you are looking at is displaying information about a specific server. The instance identified in this column is the instance in that server. </dd><dt><strong>Fully Connected</strong> </dt>
<dd>Lists (by instance ID number) the instances to which the instance in the row is fully, partially, or not connected. </dd><dt><strong>Catch Up</strong> </dt>
<dd>Lists (by instance ID number) the instances which are catching up to the instance of the row. What it means for instance X to have instance Y listed as Catch Up is: (1) The required two connections have been established between X and Y; (2) One or both of the connections are in Catch Up mode (either X is trying to get Y's commits that X is missing or Y is trying to get X's commits that Y is missing, or both). </dd><dt><strong>Unconnected</strong> </dt>
<dd>Lists (by instance ID number) the instances which are not connected to the instance of the row. </dd><dt><strong>Ingest Queue Length</strong> </dt>
<dd>The number of commits this instance has collected that it must ingest.</dd><dt><strong>Commits Behind</strong> </dt>
<dd>The information passed between instances tells how many commits an instance has initiated. If Instance1 tells Instance2 that it has initiated 100 commits and Instance2 has only seen 90 commits from Instance1, it calculates it is 10 commits behind. Adding all those as yet unseen commits from all instances to the ingest queue length gives this value. Note this value is always an estimate since information on commits is sent periodically and there may be additional commits by other instances since their last report. </dd></dl><p>What buttons appear below the table depends on whether the instance displaying the page is the controlling instance or not. See the illustrations above, where the controlling instance shows all the buttons and other instances show only two (<strong>Refresh</strong> and <em>*Set Controlling Instance</em>). </p><p>We describe the buttons for the controlling instance.  Some affect the whole table and are thus always active. Others affect just one or more instances and so are inactive unless some instances are selected. The buttons are: </p><p><strong>Refresh</strong> Always available. Refresh the displayed data. Also on non-controlling instance page. </p>
<dl><dt><strong>Stop</strong> </dt>
<dd>Available when one or more instances are selected. Stop the selected instances. When an instance is stopped, data cannot be added or deleted from it and it will not receive commits from other instances. An instance is also stopped when its associated server is down or otherwise unavailable but this option stops the instance while leaving the server running. </dd><dt><strong>Start</strong> </dt>
<dd>Available when one or more instances are selected. Restarts a stopped instance. A no-op if the instance is already running. </dd><dt><strong>Add Instance</strong> </dt>
<dd>Always available. Allows adding a new instance. See <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a> above for details and an example. </dd><dt><strong>Remove Instance</strong> </dt>
<dd>Available when one or more instances are selected. Removes the selected instances. The removed instances become regular repositories containing the triples they contained when removed. In the typical use case, removed repositories are then deleted but that is neither automatic nor required. </dd><dt><strong>Settings</strong> </dt>
<dd>Always available. Displays a dialog showing the settings. See <a href="#instance-settings">Instance Settings</a> for more informaton. </dd><dt><strong>Set Controlling Instance</strong> </dt>
<dd>Available when exactly one instance is selected. Makes that instance the controlling instance. A no-op when the selected instance is already the controlling instance. Also on non-controlling instance page. </dd></dl><p><strong>The graphs</strong> </p><p><img src="instance-graphs-agwv.jpg" alt="Replication Cluster Management Graphs"></img> </p><p>The two graphs at the bottom of the page provide information on activity.  We added about 1200 triples so the transaction rate (lower graph) had a spike around 11:46. Otherwise the graphs show not much activity in the illustration. The various buttons on the left control graph behavior. Note the <strong>Time Zone</strong> button. It can toggle between the local time, the local time where the server is running, and UTC time (Universal Coordinated Time). </p><p><a name="instance-settings"></a> </p><a name='header3-132' id='header3-132'></a><h3>Instance Settings</h3><p>The several instances in a cluster are intended to be synchronized so they have the same set of triples but when instances are making commits which add or delete triples, the instances are unlikely to actually be identical. It takes time for information about triple additions and deletions to propagate around the various instances. Eventually (assuming the instances and the associated network are actually running) synchronization will occur, but how quickly depends on factors outside the scope of this document and perhaps outside your control. </p><p>You may want to ensure that instances do not diverge too much. The settings described here allow you to control instance divergence by preventing commits until instances are sufficiently synchronized. </p><p>Default values for these setting can be specified for a server in its config file. New clusters set up by a server will use those settings unless other settings are specified on cluster creation. Once a cluster is created, the settings can be changed by the controlling instance. </p><p>There are four settings, displayed in the dialog displayed when the <strong>Settings</strong> button is clicked on the <strong>Managing Clusters</strong> dialog for the controlling instance described in the <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a> section above. </p><p><img src="instance-settings-dialog.jpg" alt="Cluster Settings"></img> </p><p>The settings are </p><p><a name="durability-setting"></a> </p>
<dl><dt><strong>Durability</strong> </dt>
<dd><p>This is a positive integer value that specifies how many instances must have a commit ingested in order for that commit to be considered durable. The count includes the instance that made the commit. </p></dd>
<dd><p>Suppose there are 9 instances in the cluster. Setting the durability to 1 means that when an instance makes a commit that commit is immediately considered durable before even being sent to any other instance (the commit will still be sent to the other instances after it's considered durable). </p></dd>
<dd><p>Setting the durability to 9 means that every instance must have ingested the commit before it's considered durable. If one or more instances are stopped at the moment then the commit will not become durable until the stopped instances are restarted. </p></dd>
<dd><p>When specifying the durability you can use three symbolic names instead of an integer: <em>min</em>, <em>max</em>, and <em>quorum</em>. <em>min</em> always means 1. <em>max</em> means the number of instances in the cluster. As the cluster grows and shrinks the meaning of <em>max</em> is automatically changed to be the number of instances in the cluster. <em>quorum</em> means the next integer greater than half the number of instances. If there are 9 instances a <em>quorum</em> is 5. If there are 8 instances <em>quorum</em> is also 5. </p></dd>
<dd><p>The default value for this setting in the server running the controlling instance is set by the <code>durability</code> configuration directive. See the <a href="daemon-config.html#multi-master-directives">Top-level directives for multi-master clusters</a> section in the <a href="daemon-config.html">Server Configuration and Control</a> document. If not specified in the configuration file, this value defaults to <code>min</code>. </p></dd></dl><p><a name="distributed-transaction-timeout-setting"></a> </p>
<dl><dt><strong>Distributed Transaction Timeout</strong> </dt>
<dd><p>A non negative integer. This is the number of seconds to wait for a commit to become durable. If durability is greater than 1, a commit initiated by an instance cannot complete until (durability - 1) other instances have indicated they have committed as well or until the timeout is reached. If the number of seconds specified by <code>Distributed Transaction Timeout</code> passes before enough instances report they have committed, the original commit completes and control returns to the caller that initiated the commit. </p></dd>
<dd><p>The default value for this setting in the server running the controlling instance is set by the <code>distributedTransactionTimeout</code> configuration directive. See the <a href="daemon-config.html#multi-master-directives">Top-level directives for multi-master clusters</a> section in the <a href="daemon-config.html">Server Configuration and Control</a> document. If not specified in the configuration file, this value defaults to 60 (seconds). </p></dd></dl><p><a name="transaction-latency-count-setting"></a> </p>
<dl><dt><strong>Transaction Latency Count</strong> </dt>
<dd><p>This value along with <strong>Durability</strong> are the controls that the administrator has in determining the level of synchronization and coupling between the various instances in the cluster. </p></dd>
<dd><p>The <strong>Transaction Latency Count</strong> is the number of transactions that each instance will allow to be non-durable and still commit another transaction. </p></dd>
<dd><p>If the Tranaction Latency Count is 4 then even if the last four commits are not yet durable we can do one more commit. If when we go to commit again we find the last 5 commits are non-durable then the commit function will signal an error (but the error can be delayed by a positive value of <a href="#transaction-latency-timeout-setting">Transaction Latency Timeout</a>). </p></dd>
<dd><p>Another example: If the Transaction Latency Count is set to zero then each commit must be durable before the next commit can be done or an error is signaled. </p></dd>
<dd><p>The default value for this setting in the server running the controlling instance is set by the <code>transactionLatencyCount</code> configuration directive. See the <a href="daemon-config.html#multi-master-directives">Top-level directives for multi-master clusters</a> section in the <a href="daemon-config.html">Server Configuration and Control</a> document. If not specified in the configuration file, this value defaults to 3. </p></dd></dl><p><a name="transaction-latency-timeout-setting"></a> </p>
<dl><dt><strong>Transaction Latency Timeout</strong> </dt>
<dd><p>The value should be a non-negative integer indicating the number of seconds that the system will wait until the <strong>Transaction Latency Count</strong> is met. If the <strong>Transaction Latency Count</strong> is not met by the <strong>Transaction Latency Timeout</strong>, then an error will be signaled. </p></dd>
<dd><p>Commits cannot complete until they become durable (the number of instances commiting is at least the value of <a href="#durability-setting">Durability</a>) or until the <a href="#distributed-transaction-timeout-setting">Distributed Transaction Timeout</a> number of seconds has passed.  There may be several commits waiting to become durable. When <a href="#transaction-latency-count-setting">Transaction Latency Count</a> plus 1 commits are waiting to become durable, attempting an additional commit will signal an error. </p></dd>
<dd><p>The error, however, will not be signaled until <code>Transaction Latency Timeout</code> seconds have passed. If during that time, sufficient commits have become durable, the commits that exceed the <code>Transaction Latency Count</code> will then proceed and (assuming the number of pending commits drops below <a href="#transaction-latency-count-setting">Transaction Latency Count</a> plus 1, no error will be signaled. If the number of pending commits remains above that value, an error will be signaled when <code>Transaction Latency Timeout</code> seconds have passed. </p></dd>
<dd><p>You may want (particularly when testing and making large batch updates) to set this value to a large number, like 3600 (which is one hour) to avoid errors caused by, say, an overloaded test machine but still catch errors when the wait is so long that a more serious problem is indicated. </p></dd>
<dd><p>The default value for this setting in the server running the controlling instance is set by the <code>transactionLatencyTimeout</code> configuration directive. See the <a href="daemon-config.html#multi-master-directives">Top-level directives for multi-master clusters</a> section in the <a href="daemon-config.html">Server Configuration and Control</a> document.  If not specified in the conifguration file, this value defaults to 0 (seconds). </p></dd></dl><a name='header3-164' id='header3-164'></a><h3>Cluster operations you cannot do with AGWebView</h3><p>There are certain cluster operations that cannot be done using AGWebView. Instead, you should use the <a href="#agtool-repl">agtool repl</a> command interface. These things include </p>
<ul>
<li><p>Quiesce the cluster (stop all commits until the cluster is in sync)   (see <a href="#agt-quiesce">agtool repl quiesce</a>) </p></li>
<li><p>Set instance priorities (see <a href="#agt-priority">agtool repl priority</a>)</p></li>
<li><p>Specify the retention period of transaction logs   (see <a href="#agt-retain">agtool repl retain</a>) </p></li></ul><p><a name="agtool-repl"></a> </p><a name='header2-170' id='header2-170'></a><h2>Using the agtool repl command</h2><p>The <strong>agtool repl</strong> command can be used to create and manage multi-master clusters. <strong>agtool</strong> is the general command tool for AllegroGraph (see the <a href="agtool.html">agtool General Command Utility</a> document). </p><p>The general form of the <strong>agtool repl</strong> command is </p>
<pre><code>agtool repl COMMAND [ OPTIONS ] ... </code></pre><p>where COMMAND is one of: </p>
<ul>
<li><a href="#agt-controlling-instance">controlling-instance</a> - Set the controlling instance of a cluster.</li>
<li><a href="#agt-create-cluster">create-cluster</a> - Create the first instance of a cluster.</li>
<li><a href="#agt-grow-cluster">grow-cluster</a> - Add an instance to the cluster.</li>
<li><code>help</code> - Display repl usage.</li>
<li><a href="#agt-priority">priority</a> - Specify the priority for retrieving commits.</li>
<li><a href="#agt-quiesce">quiesce</a> - Control the coordination of replication in the cluster.</li>
<li><a href="#agt-remove">remove</a> - Remove an instance from the cluster.</li>
<li><a href="#agt-retain">retain</a> - Specify the retention period of transaction logs.</li>
<li><a href="#agt-settings">settings</a> - Set cluster parameters.</li>
<li><a href="#agt-start">start</a> - Restart a stopped cluster instance.</li>
<li><a href="#agt-status">status</a> - Get the status of an instance of the cluster.</li>
<li><a href="#agt-stop">stop</a> - Stop a cluster instance.</li></ul><p><a name="agtool-spec-arg"></a> </p><a name='header3-188' id='header3-188'></a><h3>The SPEC argument to agtool repl</h3><p>Most <strong>agtool repl</strong> commands take a <em>SPEC</em> argument that specifies the cluster instance to act upon. The form of this argument is </p>
<pre><code>http(s):[//[USER:PASSWORD@]HOST[:PORT]][/catalogs/CATALOG]/repositories/REPO </code></pre><p>Example: </p>
<pre><code>http://test:xyzzy@host:43160/repositories/my-repo </code></pre><p>The user is <code>test</code> with password <code>xyzzy</code>, and the repo is <code>my-repo</code> on host <code>host</code> listening on port <code>43160</code>. This repository is in the root catalog (the default so no catalog is specified). The scheme is <code>http</code>. </p><p><a name="agt-controlling-instance"></a> </p><a name='header3-195' id='header3-195'></a><h3>Using agtool to set the controlling instance</h3><p>This can also be done in AGWebView (see <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a>). The <strong>agtool repl controlling-instance</strong> usage is </p>
<pre><code>agtool repl controlling-instance [ OPTION ] ... SPEC INSTANCE_NAME </code></pre><p>Options are <code>--force</code> (force operation to complete even if all requirements are not met) and <code>-v</code>/<code>--verbose</code> (more verbose output). <code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a> and must specify the controlling instance of the cluster. The instance name (in the cluster) must be included and <em>cannot</em> be the controlling instance name. <code>INSTANCE_NAME</code> can be the name or a SPEC value denoting the instance to become the controlling instance. If the controlling instance is down, <code>INSTANCE_NAME</code> must be a SPEC value denoting the instance to become the controlling instance and <code>--force</code> should be specified. </p><p><a name="agt-create-cluster"></a> </p><a name='header3-200' id='header3-200'></a><h3>Using agtool to create a cluster</h3><p>This can also be done in AGWebView but using a somewhat different sequence of commands (see the note below and see <a href="#convert-using-webview">Using AGWebView to convert a repository to a multi-master instance</a>. The <strong>agtool repl create-cluster</strong> usage is </p>
<pre><code>agtool repl create-cluster [ OPTION ] ... SPEC [ INSTANCE_NAME ] </code></pre><p>Options are </p>
<ul>
<li><code>--group</code> (see below)</li>
<li><code>-v</code>/<code>--verbose</code> if supplied, causes more verbose output</li>
<li><code>--if-exists</code> if supplied, specifies what to do when the repo   to convert already exists. See below for details. </li>
<li><code>--force</code> if supplied with value <code>true</code>, forces the conversion   of an existing repo to a multi-master instance even if there are   unsuitable encoded id templates. See below for details. </li></ul><p><code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a>. The instance name (in the cluster) can be included. An instance name will be assigned if it is not specified. The instance will become the first instance of the cluster and hence the controlling instance. </p><h4>The --if-exists and --force options</h4><p>The typical use of <strong>agtool repl create-cluster</strong> is to have it create a new repository as the first (and controlling) instance of a cluster. However, you can convert an existing repository to become the first instance of a cluster. To do so, specify <code>use</code> as the value of the <code>--if-exists</code> option. </p><p>The allowable values for <code>--if-exists</code> are: </p>
<ul>
<li><code>supersede</code> - Delete the existing repo and all its contents and recreate   it with no contents as the first instance of the new cluster. Use the option   with care. We recommend backing the repo up prior to superseding it. </li>
<li><code>overwrite</code> - Same as <code>supersede</code>.</li>
<li><code>error</code> - Signal an error of the repo specified by <code>SPEC</code> exists.   This is the default value. </li>
<li><code>use</code> - Use the existing repo with its contents as the first instance   of the new cluster. See the section <a href="#encoded-ids">Issues with encoded-ids</a>   below for requirements on encoded ids defined in the repo. </li></ul><p>If the system will not convert an existing repo when <code>--if-exists use</code> is specified because of encoded id problems (see <a href="#encoded-ids">Issues with encoded-ids</a> below), you can force the conversion by specifying <code>--force true</code>: </p>
<pre><code>agtool repl create-cluster --if-exists use --force true \  
       http://test:xyzzy@host:43160/repositories/my-repo </code></pre><p>will then convert the existing repo <strong>my-repo</strong> to be the first instance in a cluster whether or not there are issues with encoded ids. </p><h4>The --group option</h4><p>The group option can also be specified using AGWebView (see <a href="#convert-using-webview">the description of converting a repository to the initial instance</a> and <a href="#managing-clusters-webview">the description of adding an instance</a> above, where the dialogs used to convert or create instances have a <strong>Group</strong> field). </p><p>Cluster instances may be in a group but need not be, so <code>--group</code> need not be specified. Groups have names which identify them. Group names must be distinct from instance names. Since this is the first cluster instance, the group specified will be created with this instance as its first member. See <a href="#agt-priority">priority below</a> for information on how groups are used. </p><p><a name="encoded-ids"></a> </p><h4>Issues with Encoded IDs</h4><p>Encoded IDs can be used to generate unique URIs. The user registers a prefix template and then can call <a href="lisp-reference.html#next-encoded-upi-for-prefix" title="description of next-encoded-upi-for-prefix">next-encoded-upi-for-prefix</a> to get the new unique identifier. See the <a href="encoded-ids.html">Encoded IDs</a> document for more information. </p><p>In multi-master replication clusters, however, the prefix templates <strong>must</strong> specify exactly 2^60 different identifiers. (Outside of multi-master custers, prefix templates can specify fewer unique idenifiers.) </p><p>Consider an example similar to one in the <a href="encoded-ids.html">Encoded IDs</a> document: </p>
<pre><code>[0-9]{3}-[a-z]{3}-[0-9]{4} </code></pre><p>That calls for [three digits]-[three letters]-[four digits], like <em>143-afg-0126</em>. There are 10^3<em>26^3</em>10^4 = 175,760,000,000 unique identifiers.  But 2^60 is 1,152,921,504,606,846,976, so that template will not work for calls to <a href="lisp-reference.html#next-encoded-upi-for-prefix" title="description of next-encoded-upi-for-prefix">next-encoded-upi-for-prefix</a> in a multi-master instance. </p><p>The special template "plain" simply runs through the integers from 0 to (2^60 -1) and is recommended if your goal is unique URIs. Other templates that work are: </p>
<pre><code>[a-p]{15}  
[0-7]{10}-[0-7]{10} </code></pre><p>Consider the first: there are 16 letters from <code>a</code> to <code>p</code> and the template is 15 letters long so there are (2^4)^15 = 2^60 possibilities. </p><p>Once a repository is an instance of a multi-master cluster, you can define new Encoded IDs with unsuitable templates but calls to <a href="lisp-reference.html#next-encoded-upi-for-prefix" title="description of next-encoded-upi-for-prefix">next-encoded-upi-for-prefix</a> will signal an error. </p><p>The mere existence of an inadequate template will not cause a problem, nor will using existing UPIs created before the repository became a cluster instance, just trying to create a new UPI with <a href="lisp-reference.html#next-encoded-upi-for-prefix" title="description of next-encoded-upi-for-prefix">next-encoded-upi-for-prefix</a> or its equivalents in other languages such as Java or using the REST interface. </p><p>Note that the required number of distinct strings generated by a template may change in future releases. The <a href="release-notes.html">Release Notes</a> will describe any the change should it occur. </p><h4>Differences between creating a cluster with agtool and with AGWebView</h4><p>When you create a cluster in AGWebView, you first create a repo and then convert it to a cluster repo. (You can have added triples before conversion, but you always start with an existing repository and its contents are unchanged by the conversion.) When you use <strong>agtool repl create-cluster</strong>, you are typically creating a new, empty repository as a cluster repo. You can use an existing repo like in AGWebView by specifying <code>--if-exists use</code>. Otherwise, if `SPEC' names an existing repo containing triples, the system will delete it and create a new, empty repo with the same name. </p><p><a name="agt-grow-cluster"></a> </p><a name='header3-238' id='header3-238'></a><h3>Using agtool to add an instance to a cluster</h3><p>This can also be done in AGWebView (see <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a>). The <strong>agtool repl grow-cluster</strong> usage is </p>
<pre><code>agtool repl grow-cluster [ OPTION ] ... SPEC TARGET-SPEC [ INSTANCE_NAME ] </code></pre><p><code>SPEC</code> and <code>TARGET-SPEC</code> are as described <a href="#agtool-spec-arg">above</a>. <code>SPEC</code> indentifies the controlling instance of an existing cluster and <code>TARGET-SPEC</code> identifies the new cluster instance. If <code>TARGET-SPEC</code> identifies an existing repository, that repository will be deleted and reconstituted as a cluster instance. If <code>INSTANCE-NAME</code> is not specified, one will be assigned. </p><p>The OPTIONs are <code>--group</code> and <code>-v</code>/<code>--verbose</code> (more verbose output). <code>--group</code> is optional. It specifies the group the new instance should be in (if any). The value can be an existing group name (the instance will go in that group) or a new name (a new group will be created). See <a href="#agt-priority">priority below</a> for information on how groups are used. </p><p><a name="agt-priority"></a> </p><a name='header3-244' id='header3-244'></a><h3>Using agtool to specify cluster instance priorities</h3><p>This cannot be done in AGWebView. The <strong>agtool repl priority</strong> usage is </p>
<pre><code>agtool repl priority [ OPTION ] ... SPEC </code></pre><p>Options are <code>--relation</code> (see below) and <code>-v</code>/<code>--verbose</code> (more verbose output). <code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a>. </p><p>Priorities specify how commits from different instances will be processed by a particular instance. See the section <a href="#repo-priority">Repository Priority</a> for information on how priorities are used. </p><p>The <code>--relation</code> argument value should be a string of the form <code>name1,name2,value</code> where nameX is a group or instance name. </p><p>Note that two instances in the same group have a default priority of 20 with respect to each other. This can be overridden. Two instances not in the same group (perhaps not in any group) and not having a defined priority have default priority 10. </p><p><a name="agt-quiesce"></a> </p><a name='header3-252' id='header3-252'></a><h3>Using agtool to quiesce a cluster so all instances sync up</h3><p>This cannot be done in AGWebView. The <strong>agtool repl quiesce</strong> usage is </p>
<pre><code>agtool repl quiesce [ OPTION ] ... SPEC </code></pre><p><code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a>. </p><p><code>OPTIONS</code> include one required option, <code>--action</code>, and two optional options, <code>--timeout</code> (seconds to wait for the operation to complete) and <code>-v</code>/<code>--verbose</code> (more verbose output). </p><p><code>--action</code> specifies the quiesce action. It must be one of: </p>
<ul>
<li><p><strong>stop</strong>: Put the specified instance in a state where attempting a new commit causes an error. The instance then waits to be connected to all other instances so all of its existing commits are distributed. Once all instances are in this state (see  --<code>action=stopall</code> below) then we know that all instances are caught up with all other instances. This action can timeout. </p></li>
<li><p><strong>stopall</strong>: This does the operation of <code>action=stop</code> on all instances    This action can timeout. </p></li>
<li><p><strong>start</strong>: This allows the specified instance to accept commits.</p></li>
<li><p><strong>startall</strong>: This does the <code>--action=start</code> on all instances.</p></li>
<li><p><strong>restartall</strong>: This does the <code>--action=stopall</code> followed by the      <code>action=startall</code>. </p></li>
<li><p><strong>ready</strong>: This is like <strong>stop</strong> except it doesn't prevent commits from occuring.  It just checks that this instance is connected to all other instances and that it has no more commits to distribute. </p></li>
<li><p><strong>readyall</strong>: This does <code>--action ready</code> on all instances.</p></li></ul><p>Each instance of the cluster runs independently bringing itself up to date with other instances as rapidly as it can.  There may be a time when you need all instances to get completely in sync and have a commit distributed to all of them before anything further is changed. </p><p><em>Quiescing the cluster</em> means stopping all instances from commiting and waiting until all instances are up to date.  All of the quiesce operations will only work if all cluster instances are running.  If any have been stopped or if an agraph server is simply unreachable at the moment the quiesce operation will wait until all instances are running or until the specified timeout is reached. Specifying a timeout is always a good idea as otherwise the system can wait forever. </p><p><a name="agt-remove"></a> </p><a name='header3-268' id='header3-268'></a><h3>Using agtool to remove a cluster instance</h3><p>This can also be done in AGWebView (see <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a>). The <strong>agtool repl remove</strong> usage is </p>
<pre><code>agtool repl remove [ OPTION ] ... SPEC  INSTANCE_NAME </code></pre><p>Options are <code>--force</code> (force operation to complete even if all requirements are not met), <code>--timeout</code> (number of seconds to wait for the operation to complete) and <code>-v</code>/<code>--verbose</code> (more verbose output). <code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a> and must specify the controlling instance of the cluster. The instance name (in the cluster) must be included and <em>cannot</em> be the controlling instance name. <code>INSTANCE_NAME</code> can be the name or a SPEC value denoting the instance to be removed.  The instance identified by <code>INSTANCE_NAME</code> is removed from the cluster. (Note that the format of this command differs from other commands. <code>SPEC</code> and <code>INSTANCE_NAME</code> both must be supplied and must refer to different instances: <code>SPEC</code> to the controlling instance, <code>INSTANCE_NAME</code> to the instance to be removed, which cannot be the controlling instance.) </p><p><a name="agt-retain"></a> </p><a name='header3-273' id='header3-273'></a><h3>Using agtool to specify the transaction log retention period</h3><p>This cannot be done in AGWebView. The <strong>agtool repl retain</strong> usage is </p>
<pre><code>agtool repl retain [ OPTION ] ... SPEC </code></pre><p><code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a>. </p><p><code>OPTIONS</code> are the following, which specify how transaction logs should be retained. </p>
<dl><dt><strong>--must-retain MUST-RETAIN</strong> </dt>
<dd>The ID of the minimum transaction log that must be retained. A value of -1 will turn off retention based on this value. </dd><dt><strong>--since-time SINCE-TIME</strong> </dt>
<dd>All transactions since the given date must be retained. <code>SINCE-TIME</code> must be an <a href="https://www.iso.org/iso-8601-date-and-time-format.html">ISO date</a>, "now", or a <a href="https://franz.com/support/documentation/10.1/ansicl/subsubse/universa.htm">universal time</a>. A value of -1 will turn off retention based on this value. </dd><dt><strong>--time-interval TIME-INTERVAL</strong> </dt>
<dd>Transaction log entries between now and time-interval seconds ago will be retained. You can use this to retain, say, the last 24 hours of entries by specifying 86400 seconds.  A value of -1 turns off retention based on this value. </dd></dl><p>There is also a <code>-v</code>/<code>--verbose</code> option (more verbose output). </p><p><a name="agt-settings"></a> </p><a name='header3-286' id='header3-286'></a><h3>Using agtool to set cluster parameters</h3><p>This can also be done in AGWebView (see <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a>). The <strong>agtool repl settings</strong> usage is </p>
<pre><code>agtool repl settings [ OPTION ] ... SPEC [ INSTANCE_NAME ] </code></pre><p>Options allow you to specify which settings you want to set. See <a href="#instance-settings">Instance Settings above</a> for more information on these values. The choices are: </p>
<dl><dt><strong>--distributed-transaction-timeout SECONDS</strong> </dt>
<dd>Set the distributed transaction timeout in seconds for all instances.</dd><dt><strong>--durability DURABILITY</strong> </dt>
<dd>Set the durability for the cluster: an integer or <code>min</code>,<code> max</code> or <code>quorum</code>.</dd><dt><strong>--transaction-latency-count COUNT</strong> </dt>
<dd>Set the transaction latency count for all instances.</dd><dt><strong>--transaction-latency-timeout SECONDS</strong> </dt>
<dd>Set the transaction latency timeout in seconds for all instances.</dd></dl><p>There is also a <code>-v</code>/<code>--verbose</code> option (more verbose output). <code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a> and must be the controlling instance. </p><p><a name="agt-start"></a> </p><a name='header3-300' id='header3-300'></a><h3>Using agtool to start an instance</h3><p>This can also be done in AGWebView (see <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a>). The <strong>agtool repl start</strong> usage is </p>
<pre><code>agtool repl start [ OPTION ] ... SPEC </code></pre><p>The single option is <code>-v</code>/<code>--verbose</code> (more verbose output). <code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a> and need not be the controlling instance.  The command is sent directly to the specified instance which is then restarted. This is a no-op if it is already running. </p><p><a name="agt-status"></a> </p><a name='header3-305' id='header3-305'></a><h3>Using agtool to get the status of an instance</h3><p>The <strong>agtool repl status</strong> usage is </p>
<pre><code>agtool repl status [ OPTION ] ... SPEC </code></pre><p>The single option is <code>-v</code>/<code>--verbose</code> (more verbose output). <code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a>. The instance status is printed. Note the status is quite verbose and is intended to assist debugging so may not be meaningful to users. This can also be done in AGWebView (see <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a>). Not all the information is displayed in AGWebView. </p><p><a name="agt-stop"></a> </p><a name='header3-310' id='header3-310'></a><h3>Using agtool to stop a cluster instance</h3><p>This can also be done in AGWebView (see <a href="#managing-clusters-webview">Managing clusters in AGWebView once created</a>). The <strong>agtool repl stop</strong> usage is </p>
<pre><code>agtool repl stop [ OPTION ] ... SPEC </code></pre><p>Options are <code>--force</code> (force operation to complete even if all requirements are not met), <code>--timeout</code> (seconds to wait for the operation to complete) and <code>-v</code>/<code>--verbose</code> (more verbose output). <code>SPEC</code> is as described <a href="#agtool-spec-arg">above</a>.  It identifies the instance to be stopped, not the controlling instance (unless that is the instance to be stopped). The specified instance is stopped (no longer can make or process commits). This is a no-op if it is already stopped. </p><p><a name="repl-features"></a> </p><a name='header2-315' id='header2-315'></a><h2>Details on Certain Multi-master Replication Features</h2><p>In this section, we describe certain features mentioned elsewhere in this document but requiring a longer description best put in a single location. </p><p><a name="repo-priority"></a> </p><a name='header3-318' id='header3-318'></a><h3>Repository priorities</h3><p>All repositories in a replication cluster share information about their own commits and those of other repositories that they know about. This ensures that cases like the following are properly handled: REPO-3 is down when REPO-1 makes a commit, and then REPO-1 goes down followed by REPO-3 coming up. REPO-2 -- which has been running the whole time -- will inform REPO-3 of REPO-1's commits. </p><p>It can sometimes happen that a repository falls behind (perhaps it was offline or running slowly during a period of great activity). In that case, it finds that the same commit information is available from many other repositories and the question arises: from which of the repositories should it take the information? </p><p>Suppose in your setup you have two data centers, one in Chicago in the USA and one in Singapore. Each has 10 machines each running an AllegroGraph server. You have a multi-master replication cluster with instances on every one of the 20 servers. </p><p>If an instance in Singapore falls behind (say, because its server is taken down for maintenance for 24 hours during which other instances made many commits), it will be far behind when it comes back up. Should it get information from an instance in Chicago, which is around the world, or another instance in Singapore, down the hall? Obviously it should get as much information as possible from its neighboring instances. </p><p>Where instances prefer to get information is determined by the <em>priority</em> feature. When the same information is available from many instances, it will be taken from the instance whose priority is highest. </p><p>Here is a more detailed example. All instances in a replication cluster are named <em>instN</em> (where <em>N</em> is an integer). </p><p>Instance <em>inst1</em> needs a certain commit and it finds that the commit is available on <em>inst2</em>, <em>inst3</em> and <em>inst4</em>. </p><p>Which instance should <em>inst1</em> ask for the commit? </p><p>The system calculates the priority between the pairs </p>
<pre><code>inst1 inst2  
inst1 inst3  
inst1 inst4 </code></pre><p>and the highest value wins. </p><p>Groups also come into play when computing the priority. Each instance is in zero or one group. </p><p>The set of group names are distinct as are the set of instance names. Furthermore group names are distinct from instance names. Thus a particular name is either an instance name, a group name or neither. </p><p>The priorities are specified as a set of triples: </p>
<pre><code>(name1, name2, priority) </code></pre><p>meaning <em>for name1 the priority of getting commits from name2 is priority</em>. Again <em>name1</em> and <em>name2</em> can be names of instances or groups. </p><p>The priority will determine whether we use this connection to retrieve a commit we need. </p><p>For each pair of instances <em>inst1</em> and <em>inst2</em> we compute the priority value this way, checking each of the possibilities listed in order and stopping when we find a possibility that matches: </p>
<ul>
<li><p>If there's a triple <code>["inst1", "inst2", value]</code> then that is the priority value for <em>inst1</em> getting commits from <em>inst2</em>. </p></li>
<li><p>If <em>inst1</em> is a member of group <em>group1</em> and there is a triple <code>["group1", "inst2", value]</code> then that is the value. </p></li>
<li><p>If <em>inst2</em> is a member of <em>group2</em> and there is a triple <code>["inst1",   "group2", value]</code> then that is the value. </p></li>
<li><p>If <em>inst1</em> is in <em>group1</em> and <em>inst2</em> is in <em>group2</em> and there is a triple <code>["group1", "group2", value]</code> then that is the value. </p></li>
<li><p>If none of the above match then the default value is 10.</p></li></ul><p>For each group there's a default triple (that can be overridden) of </p>
<pre><code>["group1","group1", 20] </code></pre><p>meaning, by default members of the same group have a priority of 20 between them.  Two instances that are in no group are not considered to be in the same group. </p><p>Priorities can be set between instances and/or groups with <a href="#agt-priority">agtool</a> or using the <a href="#rest-interface">REST interface</a>. They cannot be set with AGWebview. </p><p><a name="rest-interface"></a> </p><a name='header2-347' id='header2-347'></a><h2>The REST interface for multi-master replication</h2><p>Multi-master replication clusters can be created and managed using the REST interface. There is an introduction to multi-master replication REST commands in the <a href="http-protocol.html#multi-master">Multi-master replication</a> section of the <a href="http-protocol.html">REST/HTTP interface</a>. All REST interface commands are described in the <a href="http-reference.html">HTTP reference</a> document. </p></div> </div>  <p><script> $.each($("#agmenu ul li a"), function(ignore, link) {   var anchor = $(link);   anchor.parent().on("click", function () { window.location = anchor.prop("href"); }); }); $("#agmenu").css("display", "block") </script> </p> 
</body>
</html>
